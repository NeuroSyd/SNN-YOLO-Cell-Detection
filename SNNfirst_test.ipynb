{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import lava.lib.dl.slayer as slayer\n",
    "import lava.lib.dl.slayer.io as sio\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "Height = 720\n",
    "Width = 1280\n",
    "df_f = 2\n",
    "roix =[int(0.67*Width)//df_f,int(0.70*Width)//df_f]\n",
    "roiy = [int(0*Height)//df_f,int(1*Height)//df_f]\n",
    "\n",
    "width = roix[1]-roix[0]\n",
    "height = roiy[1] - roiy[0]\n",
    "\n",
    "heightoffset = roiy[0]\n",
    "widthoffset = roix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "Dataset_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for SNN\n",
    "class FCDataset(Dataset):\n",
    "    '''\n",
    "        pathlist: list of path to the classes\n",
    "        sampling_time: total duration of the event file\n",
    "        sample_bins: number of bins to sample the event file\n",
    "        x: width of the sensor\n",
    "        y: height of the sensor\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, pathlist=[],\n",
    "        sampling_time=0.5e-6, sample_bins=100,x=128,y=128):\n",
    "            super(FCDataset, self).__init__()\n",
    "            self.classnum = len(pathlist)\n",
    "            self.pathlist = pathlist\n",
    "            self.sampling_time = sampling_time\n",
    "            self.sample_bins = sample_bins\n",
    "            self.data = []\n",
    "            self.label = []\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            for idx, path in enumerate(pathlist):\n",
    "                eventflielist = glob.glob(f'{path}/xypt_*.npy')\n",
    "                for eventfile in eventflielist:\n",
    "                    event = sio.read_np_spikes(eventfile,fmt='xypt',time_unit=0.0000005)\n",
    "                    self.data.append(event)\n",
    "                    self.label.append(idx)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "            event = self.data[idx]\n",
    "            #print(event)\n",
    "            spike = event.fill_tensor(\n",
    "                torch.zeros(2, self.y, self.x, self.sample_bins),\n",
    "                sampling_time=self.sampling_time,\n",
    "                )\n",
    "            label = self.label[idx]\n",
    "            return spike.reshape(-1, self.sample_bins), label\n",
    "        \n",
    "pathlist = [f'{Dataset_path}numpysp2/3um_15uL_-25bias_-25off_25fo_lux+2/',\n",
    "            f'{Dataset_path}numpysp2/8um_2000D_10uL_50to10min/',\n",
    "            f'{Dataset_path}numpysp2/15um_10uL_50to10min/']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic network\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self,x,y,out):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, x*y*2, 512, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 512, 512, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 512, out, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform txyp events to xypt events\n",
    "#first time only\n",
    "for datapath in pathlist:\n",
    "    npylist = glob.glob(f'{datapath}*.npy')\n",
    "    eventlist =[]\n",
    "    for idx, name in enumerate(npylist):\n",
    "        #print(name)\n",
    "        data = np.load(name)\n",
    "        temp = []\n",
    "        mintime = min(data['t'])\n",
    "        for i in range(len(data)):\n",
    "            t = data[i]['t']-mintime\n",
    "            x = data[i]['x']-widthoffset\n",
    "            y = data[i]['y']-heightoffset\n",
    "            p = data[i]['p']\n",
    "            temp.append([x,y,p,t])\n",
    "        data = np.array(temp)\n",
    "        dirname, filename = os.path.split(name)\n",
    "        new_filename = f'xypt_{filename}'\n",
    "        new_filepath = os.path.join(dirname, new_filename)\n",
    "        np.save(new_filepath, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hx/anaconda3/envs/learning/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "x=20\n",
    "y=360\n",
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "device = torch.device('cuda') \n",
    "net = Network(x=x,y=y,out=3).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "full_dataset = FCDataset(pathlist=pathlist,sampling_time=0.0001, sample_bins=100,x=x,y=y)\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 定义数据集大小的比例\n",
    "train_size = int(0.8 * len(full_dataset))  # 假设训练集占80%\n",
    "test_size = len(full_dataset) - train_size  # 剩余的为测试集\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "spike_tensor, label = test_dataset[np.random.randint(len(test_dataset))]\n",
    "spike_tensor = spike_tensor.reshape(2, 360, 20, -1)\n",
    "testevent = sio.tensor_to_event(spike_tensor.cpu().data.numpy())\n",
    "anim = testevent.anim(plt.figure(figsize=(5, 10)), frame_rate=240)\n",
    "anim.save(f'gifs/input.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "error = slayer.loss.SpikeRate(true_rate=0.2, false_rate=0.03, reduction='sum').to(device)\n",
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            \n",
      "[Epoch 19/100]\n",
      "Train loss =     0.35395 (min =     0.34658)     accuracy = 0.99520 (max = 0.99431)  \n",
      "Test  loss =     0.63297 (min =     0.58552)     accuracy = 0.94452 (max = 0.94879) \n",
      "                                                                                                                                                                                            \n",
      "[Epoch 39/100]\n",
      "Train loss =     0.30917 (min =     0.31615)     accuracy = 0.99626 (max = 0.99742)  \n",
      "Test  loss =     0.66170 (min =     0.57850)     accuracy = 0.94346 (max = 0.95235) \n",
      "                                                                                                                                                                                            \n",
      "[Epoch 59/100]\n",
      "Train loss =     0.30690 (min =     0.29111)     accuracy = 0.99618 (max = 0.99751)  \n",
      "Test  loss =     0.71721 (min =     0.57850)     accuracy = 0.93741 (max = 0.95235) \n",
      "                                                                                                                                                                                            \n",
      "[Epoch 79/100]\n",
      "Train loss =     0.28907 (min =     0.28798)     accuracy = 0.99698 (max = 0.99813)  \n",
      "Test  loss =     0.68256 (min =     0.57850)     accuracy = 0.94452 (max = 0.95235) \n",
      "                                                                                                                                                                                            \n",
      "[Epoch 99/100]\n",
      "Train loss =     0.29740 (min =     0.28193)     accuracy = 0.99742 (max = 0.99813)  \n",
      "Test  loss =     0.59382 (min =     0.57850)     accuracy = 0.94452 (max = 0.95235) \n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, label) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    for i, (input, label) in enumerate(test_loader): # training loop\n",
    "        output = assistant.test(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "     \n",
    "    if epoch%20 == 19: # cleanup display\n",
    "        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n",
    "        stats_str = str(stats).replace(\"| \", \"\\n\")\n",
    "        print(f'[Epoch {epoch:2d}/{epochs}]\\n{stats_str}')\n",
    "    \n",
    "    if stats.testing.best_accuracy:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')\n",
    "    net.grad_flow(trained_folder + '/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
