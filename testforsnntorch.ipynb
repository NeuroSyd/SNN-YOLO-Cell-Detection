{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch.spikevision import spikedata\n",
    "import tonic\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "# create datasets\n",
    "import tonic.transforms as transforms\n",
    "import snntorch.spikeplot as splt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "import torch\n",
    "import torchvision\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size,\n",
    "                                                         time_window=1000)\n",
    "                                     ])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n",
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
    "\n",
    "# no augmentations for the testset\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m anim \u001b[39m=\u001b[39m splt\u001b[39m.\u001b[39manimator(spike_data_sample, fig, ax)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m anim\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mspike_mnist.gif\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m HTML(anim\u001b[39m.\u001b[39mto_html5_video())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#  Save as a gif\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m anim\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mspike_mnist.gif\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE/klEQVR4nO3XIRLCUBAFwR8KgeHAnCAHxuAWNxZMKhHdesVzU7vNzCwAWGvdzh4AwHWIAgARBQAiCgBEFACIKAAQUQAgogBA7v8ePh/7kTsAONj78/p541MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDbzMzZIwC4Bp8CABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgD5AuPuDgOZCBlcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdata,label  = trainset[0]\n",
    "spike_data_sample = testdata[:,1,:,:]\n",
    "spike_data_sample = torch.from_numpy(spike_data_sample).float()\n",
    "#  Plot\n",
    "fig, ax = plt.subplots()\n",
    "anim = splt.animator(spike_data_sample, fig, ax)\n",
    "anim.save(\"spike_mnist.gif\")\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "#  Save as a gif\n",
    "anim.save(\"spike_mnist.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta1 = torch.rand(1, device=device)\n",
    "beta2 = torch.rand(1, device=device)\n",
    "beta3 = torch.rand(1, device=device)\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(2, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta1, spike_grad=spike_grad, init_hidden=True,learn_beta=beta1),\n",
    "                    nn.Conv2d(12, 32, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta2, spike_grad=spike_grad, init_hidden=True,learn_beta=beta2),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(32*5*5, 10),\n",
    "                    snn.Leaky(beta=beta3, spike_grad=spike_grad, init_hidden=True,learn_beta=beta3, output=True)\n",
    "                    ).to(device)\n",
    "# this time, we won't return membrane as we don't need it\n",
    "\n",
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "#loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "loss_fn = SF.ce_count_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([312, 128, 2, 34, 34])\n",
      "torch.Size([312, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([313, 128, 2, 34, 34])\n",
      "torch.Size([309, 128, 2, 34, 34])\n",
      "torch.Size([310, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([310, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "torch.Size([310, 128, 2, 34, 34])\n",
      "torch.Size([312, 128, 2, 34, 34])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/tonic/cached_dataset.py:137\u001b[0m, in \u001b[0;36mDiskCachedDataset.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     data, targets \u001b[39m=\u001b[39m load_from_disk_cache(file_path)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mFileNotFoundError\u001b[39;00m, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m _:\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/tonic/cached_dataset.py:212\u001b[0m, in \u001b[0;36mload_from_disk_cache\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    211\u001b[0m target_list \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 212\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39;49mFile(file_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m name, _list \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m], [data_list, target_list]):\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/h5py/_hl/files.py:507\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    504\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    505\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    506\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 507\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/h5py/_hl/files.py:220\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    219\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 220\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    221\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = './cache/nmnist/train/45874_0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (data, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(trainloader)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/tonic/cached_dataset.py:150\u001b[0m, in \u001b[0;36mDiskCachedDataset.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    145\u001b[0m     save_to_disk_cache(\n\u001b[1;32m    146\u001b[0m         data, targets, file_path\u001b[39m=\u001b[39mfile_path, compress\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompress\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39m# format might change during save to hdf5, i.e. tensors -> np arrays\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[39m# We load the sample here again to keep the output format consistent.\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     data, targets \u001b[39m=\u001b[39m load_from_disk_cache(file_path)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/tonic/cached_dataset.py:221\u001b[0m, in \u001b[0;36mload_from_disk_cache\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 data \u001b[39m=\u001b[39m {\n\u001b[1;32m    217\u001b[0m                     key: f[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][()]\n\u001b[1;32m    218\u001b[0m                     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m f[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    219\u001b[0m                 }\n\u001b[1;32m    220\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m                 data \u001b[39m=\u001b[39m f[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mindex\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m][()]\n\u001b[1;32m    222\u001b[0m             _list\u001b[39m.\u001b[39mappend(data)\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_list) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/h5py/_hl/dataset.py:710\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_read_ok \u001b[39mand\u001b[39;00m (new_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    709\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fast_reader\u001b[39m.\u001b[39;49mread(args)\n\u001b[1;32m    711\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 4.69%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 2.33\n",
      "Accuracy: 14.06%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 2.31\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 3.50\n",
      "Accuracy: 19.53%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 7.03%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 7.03%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 11 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 12 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 13 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 14 \n",
      "Train Loss: 2.30\n",
      "Accuracy: 11.72%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Gradient calculation + weight update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss_val\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Store loss history for future plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/learning/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_iters = 100\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "Height = 720\n",
    "Width = 1280\n",
    "df_f = 2\n",
    "roix =[int(0.67*Width)//df_f,int(0.70*Width)//df_f]\n",
    "roiy = [int(0*Height)//df_f,int(1*Height)//df_f]\n",
    "\n",
    "width = roix[1]-roix[0]\n",
    "height = roiy[1] - roiy[0]\n",
    "\n",
    "heightoffset = roiy[0]\n",
    "widthoffset = roix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "Dataset_path = '../data/'\n",
    "pathlist = [f'{Dataset_path}numpysp2/3um_15uL_-25bias_-25off_25fo_lux+2/',\n",
    "            f'{Dataset_path}numpysp2/8um_2000D_10uL_50to10min/',\n",
    "            f'{Dataset_path}numpysp2/15um_10uL_50to10min/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_fill(pathlist):\n",
    "    for datapath in pathlist:\n",
    "        npylist = glob.glob(f'{datapath}*.npy')\n",
    "        filtered_npylist = [f for f in npylist if not os.path.basename(f).startswith('xypt')]\n",
    "        filtered_npylist = [f for f in filtered_npylist if not os.path.basename(f).startswith('tpxy')]\n",
    "        for idx,name in enumerate(filtered_npylist):\n",
    "            data = np.load(name)\n",
    "            temp = np.zeros((200,2,20,360),dtype=np.float32)\n",
    "            mintime = min(data['t'])\n",
    "            for i in range(len(data)):\n",
    "                t = data[i]['t']-mintime\n",
    "                p = data[i]['p']\n",
    "                x = data[i]['x']-widthoffset\n",
    "                y = data[i]['y']-heightoffset\n",
    "                temp[t][p][x][y] = 1\n",
    "            dirname, filename = os.path.split(name)\n",
    "            new_filename = f'tpxy_filled_{filename}'\n",
    "            new_filepath = os.path.join(dirname, new_filename)\n",
    "            np.save(new_filepath, temp)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first time only\n",
    "tensor_fill(pathlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2, 20, 360])\n"
     ]
    }
   ],
   "source": [
    "datt = np.load('../data/numpysp2/8um_2000D_10uL_50to10min/tpxy_filled_101318400us.npy')\n",
    "testensor = torch.from_numpy(datt).float()\n",
    "print(testensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for SNN\n",
    "class FCDataset(Dataset):\n",
    "    '''\n",
    "        pathlist: list of path to the classes\n",
    "        sampling_time: total duration of the event file\n",
    "        sample_bins: number of bins to sample the event file\n",
    "        x: width of the sensor\n",
    "        y: height of the sensor\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, pathlist=[],\n",
    "        sampling_time=0.5e-6, sample_bins=100,x=128,y=128):\n",
    "            super(FCDataset, self).__init__()\n",
    "            self.classnum = len(pathlist)\n",
    "            self.pathlist = pathlist\n",
    "            self.sampling_time = sampling_time\n",
    "            self.sample_bins = sample_bins\n",
    "            self.data = []\n",
    "            self.label = []\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            for idx, path in enumerate(pathlist):\n",
    "                eventflielist = glob.glob(f'{path}/tpxy_filled_*.npy')\n",
    "                for eventfile in eventflielist:\n",
    "                    event = np.load(eventfile)\n",
    "                    self.data.append(event)\n",
    "                    self.label.append(idx)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "            event = self.data[idx]\n",
    "            spike = torch.from_numpy(event).float()\n",
    "            label = self.label[idx]\n",
    "            return spike, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import surrogate\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,num_inputs, num_hidden, num_outputs, beta=0.9):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        #print(x.flatten(2).size())\n",
    "        # time-loop\n",
    "        x = x.transpose(0,1)\n",
    "        for step in range(x.size(0)):\n",
    "          cur1 = self.fc1(x[step].flatten(1)) # batch128 x 784\n",
    "          spk1, mem1 = self.lif1(cur1, mem1)\n",
    "          cur2 = self.fc2(spk1)\n",
    "          spk2, mem2 = self.lif2(cur2, mem2)\n",
    "          # store in list\n",
    "          spk2_rec.append(spk2)\n",
    "          mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0) # time-steps x batch x num_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = FCDataset(pathlist=pathlist)\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 定义数据集大小的比例\n",
    "train_size = int(0.8 * len(full_dataset))  # 假设训练集占80%\n",
    "test_size = len(full_dataset) - train_size  # 剩余的为测试集\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m anim \u001b[39m=\u001b[39m splt\u001b[39m.\u001b[39manimator(spike_data_sample, fig, ax)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m anim\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mspike_mnist.gif\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m HTML(anim\u001b[39m.\u001b[39mto_html5_video())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#  Save as a gif\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m anim\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mspike_mnist.gif\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAvCAYAAAB+KskzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABfklEQVR4nO3cMUoDQQCF4YkKsTCFBwjoBbTyQB5AsM0JvIKFlTa2gqWtN7AXBAUJYqGIBpLxCMYizGbf99VTvPJndthBrbUWACDWRusBAEBbYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACDc1rIHR9tnq9wBAKzAx/fkzzNuBgAgnBgAgHBiAADCiQEACCcGACCcGAB6bbwYld06bD0DOk0MAL01rJvl4fa8XB19tp4Cnbb0fwYA1s3PYF4uj0/L/ctOKcPH1nOgs8QA0Gsnb9NShtPWM6DTfCYAgHBiAOiN8WJU3i9uyqTutZ4Ca0UMAL0xK/PydHdQnmeD1lNgrXgzAPTG68ZXObze91gQ/snNAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEG5Qa62tRwAA7bgZAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcL/3LSotp/QaYwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#  spike_data contains 128 samples, each of 100 time steps in duration\n",
    "\n",
    "#  Index into a single sample from a minibatch\n",
    "#spike_data_sample,label = train_dataset[0]\n",
    "spike_data_sample = testensor[:,1,:,:]\n",
    "#  Plot\n",
    "fig, ax = plt.subplots()\n",
    "anim = splt.animator(spike_data_sample, fig, ax)\n",
    "anim.save(\"spike_mnist.gif\")\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "#  Save as a gif\n",
    "anim.save(\"spike_mnist.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def measure_accuracy(model, dataloader):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for data, targets in iter(dataloader):\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # forward-pass\n",
    "      spk_rec, _ = model(data)\n",
    "      spike_count = spk_rec.sum(0) # batch x num_outputs\n",
    "      _, max_spike = spike_count.max(1)\n",
    "\n",
    "      # correct classes for one batch\n",
    "      num_correct = (max_spike == targets).sum()\n",
    "\n",
    "      # total accuracy\n",
    "      running_length += len(targets)\n",
    "      running_accuracy += num_correct\n",
    "    \n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgc\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdel\u001b[39;00m net\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del net\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 1.0986120700836182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.3161450922489166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 \t Train Loss: 0.5171931982040405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.787695586681366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20 \t Train Loss: 0.19254933297634125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9196301698684692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 30 \t Train Loss: 0.09450438618659973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9157183170318604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 40 \t Train Loss: 0.035640962421894073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9672830700874329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50 \t Train Loss: 0.045064687728881836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9807965755462646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60 \t Train Loss: 0.03687649220228195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9879089593887329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 70 \t Train Loss: 0.04903246462345123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9900426864624023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 80 \t Train Loss: 0.05040236935019493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9903982877731323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [32:46<54:04:51, 1966.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 90 \t Train Loss: 0.00737003143876791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9900426864624023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [37:57<27:01:27, 992.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 \t Train Loss: 0.017157284542918205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9871976971626282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 110 \t Train Loss: 0.11571075767278671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9889758229255676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 120 \t Train Loss: 0.0022140289656817913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9935988783836365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 130 \t Train Loss: 0.014317741617560387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9928876161575317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 140 \t Train Loss: 0.04285320267081261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9943100810050964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 150 \t Train Loss: 0.008871879428625107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9932432174682617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160 \t Train Loss: 0.02745836041867733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9939544796943665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 170 \t Train Loss: 0.0003425721370149404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9928876161575317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 180 \t Train Loss: 0.0006199583294801414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [1:20:26<65:41:34, 2413.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m counter \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss_val\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest set accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmeasure_accuracy(net,\u001b[39m \u001b[39;49mtest_loader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m counter \u001b[39m==\u001b[39m \u001b[39m100\u001b[39m:\n",
      "\u001b[1;32m/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m running_accuracy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m(dataloader):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m   data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m   targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btcp/home/hx/Documents/ai/Object_detect/NC-Microparticles/testforsnntorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39m# forward-pass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Net(num_inputs=20*360*2,num_hidden=1024,num_outputs=3).to(device)\n",
    "loss = SF.ce_count_loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-3, betas=(0.9, 0.999))\n",
    "num_epochs = 100 # 60000 / 128 = 468\n",
    "counter = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=128,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128,shuffle=False)\n",
    "# Outer training loop\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in tqdm(train_batch, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, _ = net(data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "        loss_val += loss(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        # Print train/test loss/accuracy\n",
    "        if counter % 10 == 0:\n",
    "            print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "            print(f\"Test set accuracy: {measure_accuracy(net, test_loader)}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "          break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
